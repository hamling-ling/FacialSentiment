{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # This %tensorflow_version magic only works in Colab.\n",
    "  %tensorflow_version 1.x\n",
    "except Exception:\n",
    "  pass\n",
    "# For your non-Colab code, be sure you have tensorflow==1.15\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=[0.9, 1.1],\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "val_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "train_flow = train_gen.flow_from_directory(\n",
    "    '../data/input/Training',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=\"grayscale\")\n",
    "\n",
    "val_flow = val_gen.flow_from_directory(\n",
    "    '../data/input/PrivateTest',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=\"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(val_flow)\n",
    "image_batch.shape, label_batch.shape\n",
    "print(\"(min,max)=(\", np.min(image_batch), \", \", np.max(image_batch), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_flow.class_indices)\n",
    "\n",
    "labels = '\\n'.join(sorted(train_flow.class_indices.keys()))\n",
    "\n",
    "with open('model/labels.txt', 'w') as f:\n",
    "  f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "input_tensor = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
    "\n",
    "# Create the base model from the pre-trained MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    input_tensor=input_tensor,\n",
    "    include_top=False,\n",
    "    weights=None)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=1, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=9, activation='softmax')\n",
    "])\n",
    "\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable =  True\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_flow, \n",
    "    epochs=60,\n",
    "    steps_per_epoch=890, #28556/32\n",
    "    validation_data=val_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_keras_model = 'model/builtin_mobilenetv2-longrun.h5'\n",
    "model.save(saved_keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPDATA_PATH = '../data/input/Training/*/*'\n",
    "\n",
    "# A generator that provides a representative dataset\n",
    "def representative_data_gen():\n",
    "  dataset_list = tf.data.Dataset.list_files(REPDATA_PATH)\n",
    "  for i in range(100):\n",
    "    image = next(iter(dataset_list))\n",
    "    image = tf.io.read_file(image)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.cast(image / 255., tf.float32)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    yield [image]\n",
    "\n",
    "def relu6(x):\n",
    "  return K.relu(x, max_value=6)\n",
    "\n",
    "# This code not work because of \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(saved_keras_model,\n",
    "                                                         custom_objects={'relu6': relu6})\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model/builtin_mobilenetv2-longrun.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(val_flow)\n",
    "\n",
    "logits = model(batch_images)\n",
    "prediction = np.argmax(logits, axis=1)\n",
    "truth = np.argmax(batch_labels, axis=1)\n",
    "\n",
    "keras_accuracy = tf.keras.metrics.Accuracy()\n",
    "keras_accuracy(prediction, truth)\n",
    "\n",
    "print(\"Raw model accuracy: {:.3%}\".format(keras_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_tensor(interpreter, input):\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  tensor_index = input_details['index']\n",
    "  scale, zero_point = input_details['quantization']\n",
    "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "  # The input tensor data must be uint8: within [0, 255].\n",
    "  input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
    "\n",
    "def classify_image(interpreter, input):\n",
    "  set_input_tensor(interpreter, input)\n",
    "  interpreter.invoke()\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "  output = interpreter.get_tensor(output_details['index'])\n",
    "  # Because the model is quantized (uint8 data), we dequantize the results\n",
    "  scale, zero_point = output_details['quantization']\n",
    "  output = scale * (output - zero_point)\n",
    "  top_1 = np.argmax(output)\n",
    "  return top_1\n",
    "\n",
    "interpreter = tf.lite.Interpreter('model/builtin_mobilenetv2-longrun.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Collect all inference predictions in a list\n",
    "batch_prediction = []\n",
    "batch_truth = np.argmax(batch_labels, axis=1)\n",
    "\n",
    "for i in range(len(batch_images)):\n",
    "  prediction = classify_image(interpreter, batch_images[i])\n",
    "  batch_prediction.append(prediction)\n",
    "\n",
    "# Compare all predictions to the ground truth\n",
    "tflite_accuracy = tf.keras.metrics.Accuracy()\n",
    "tflite_accuracy(batch_prediction, batch_truth)\n",
    "print(\"Quant TF Lite accuracy: {:.3%}\".format(tflite_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
